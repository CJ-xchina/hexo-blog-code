---
title: 模式识别与机器学习
date: 2023年9月13日12:48:36
tags: 
    - AI
    - 课堂学习
categories: 前沿技术
keywords:
description: 
top_img: https://w.wallhaven.cc/full/yj/wallhaven-yjvxeg.jpg
comments:
cover: https://w.wallhaven.cc/full/yj/wallhaven-yjvxeg.jpg
toc: 
toc_number:
toc_style_simple:
copyright:
copyright_author:
copyright_author_href:
copyright_url:
copyright_info:
mathjax:
katex:
aplayer:
highlight_shrink:
aside:
---

<meta name="referrer" content="no-referrer"/>

# 概述

==模式==：在于时间和空间可观测的物体，可以区别其是否相同或相似。
==模式的直观特性==：

* 可观测性
* 可区分性
* 相似性

==模式识别与机器学习的目的==：利用计算机对对象进行分类，在错误概率最小的条件下，使识别的结果与客观物体相符合，即： Y = F(X) ，X为输入特征集，Y为机器识别类别的标号集，F是模式识别判定算法。
得到F的方法，就是通过大量训练数据学习，提升F预测结果。

==机器学习的概念==：研究如何构造理论、算法和计算机系统，让机器通过数据中学习后进行分类、识别事物、推理预测和预测未来等。

==模式识别系统目标==：在特征空间与解释空间之间找到一种映射关系，该关系成为假设。

* 特征空间：从模式得到的对分类有用的度量、属性或基元构成的空间。
* 解释空间：所属类别的集合。

获得假说的两种方式：

1. 监督学习、概念驱动或归纳假说
2. 非监督学习、数据驱动或演绎假说

# 第二章、生成式分类器

## 2.1 贝叶斯判别准则

在给定观测值$x$的情况下，判断其属于$w_1$类还是$w_2$，作出某次判断时的错误率是：
$$
P(error|x)=\begin{cases}P(\omega_{1}|x),x\in\omega_{2}\\P(\omega_{2}|x),x\in\omega_{1}\end{cases}
$$
在贝叶斯判别准则下，若$P( \omega _ {1}  |x)>P(  \omega _ {2}  |x)$ 则判定类别$x\in w_1$，否则判别$x\in w_2$ ，该判别准则也就是要确定$x$是属于哪一类，这要取决于$x$来自哪一类的概率更大，即：

<img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230919173027970.png" alt="image-20230919173027970" style="zoom: 67%;" />

回顾一下**全概率公式**，设样本空间有随机事件$w_1，w_2，w_3，....w_n$，这$n$个随机事件两两不相容，即$w_1w_2=\phi$，且满足$\bigcup_{i=1}^{n}w_i=\Omega$，设$x$为$\Omega$中的一个事件，则全概率公式为：
$$
P(x) = \sum_{i=1}^{n}P(x|w_i)P(w_i)
$$
回顾一下贝叶斯公式，样本空间有随机事件$w_1，w_2，w_3，....w_n$，这$n$个随机事件两两不相容，即$w_1w_2=\phi$，且满足$\bigcup_{i=1}^{n}w_i=\Omega$，设$x$为$\Omega$中的一个事件，贝叶斯公式为：
$$
P(w_i|x)=\frac{P(x|w_i)P(w_i)}{\sum_{j=1}^{n}P(x|w_j)P(w_j)}=\frac{P(x|w_i)P(w_i)}{P(x)}
$$
根据贝叶斯公式：$P(  \omega _ {1}  |x) = \frac{P(x|w_1)P(w_1)}{P(x)}$  $P(  \omega _ {2}  |x) = \frac{P(x|w_2)P(w_2)}{P(x)}$，而两项中$P(x)$均为全概率公式相等，因此比贝叶斯判别式为：

<img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230919180132020.png" alt="image-20230919180132020" style="zoom: 50%;" />

化为：

<img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230919180553440.png" alt="image-20230919180553440" style="zoom:67%;" />

其中$P(x|w_i)$为**似然函数** ，表示特征向量x来自类别$ω_i$的概率，图中还有似然比与判决阈值的定义。

## 2.2 贝叶斯最小风险判别

### 代价矩阵

如果分类器判别$x$是属于$w_j$类，但它实际上来自$w_i$类，也就是说分类器判断失败，错误的分类会产生'损失'或'代价'，将代价存储在一个矩阵$L_{i，j}$中，$L_{i，j}$代表分类器判定为$w_j$类其实其属于$w_i$类这种情况所产生的代价，更具体来说:

- $L_{i，j}$是个矩阵，其中$i$表示样本的真实类，$j$表示分类器预测的类。
- 当$i=j$时，分类正确，$L_{i，j}$取 0 或负值，表示没有损失。
- 当$i≠j$时，分类错误$L_{i，j}$取正值，其数值表示错误分类的程度。

例如:

- 对数字识别问题来说，将5错误分类为3的损失可能比将5错误分类为0的损失小。
- 对病人诊断问题来说，将癌症患者错误判断为普通感冒的损失会比相反情况要大。

所以$L_{i，j}$可以对应不同类型的错误设置不同的“代价值”，从而形成一个“损失矩阵”。

通过最小平均条件风险进行分类，实际上是在考虑各种错误的$L_{i，j}$值，选择分类误差最小的方案。这样可以有效减轻高风险错误的影响。

所以简单来说，$L_{i，j}$反映了分类器在不同类型分类错误下的“损失级别”，是贝叶斯最小风险分类的一个重要参数。

---

### 平均条件风险

**平均条件风险(average conditional risk)**：样本$x$共有$M$种可供选择的类别，其所属真实类为$w_j$，将其分类到类$w_i$时的平均风险，也即：
$$
r_j=  \sum _ {i=1}^ {M} L_ {ij}  P(  w_{i}|x) = \frac{\sum^{M}_{i=1}L_{ij}P(x|w_i)P(w_i)}{P(x)}
$$
换句话说，平均条件风险考虑了:

1. 各种错误分类的损失程度($L_{ij}$表示)
2. 不同错误发生的概率($P(w_i|x)$表示错误分类到$w_i$的概率)
3. 对所有可能错误进行加权求和

通常来说，最小平均条件风险分类器就是：对每个样本$x$计算所有类的平均条件风险$r_j，$将样本$x$指定为平均条件风险最小的那一个类。

### 例题解析

题目：

![image-20230919183559815](https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230919183559815.png)

- 信道输入信号只可能为0或1。
- 信道传输过程中会加入高斯噪声。噪声的均值是0,方差是$σ^2$。
- 信道输出为x。
- **高斯分布(Gaussian Distribution)**也称正态分布

解析：

#### **方法一（平均条件风险）：**

噪声是服从$N(0, σ^2)$高斯分布，输入为0时,输出x ~ $N(0, σ^2)$;输入为1时,输出x ~$ N(1, σ^2)$

定义：**$w_1$：输出为0**；**$w_2$：输出为1**。用贝叶斯判别条件分析：**设信号送$0$的先验概率为**
**$P(0)$,送$1$的先验概率为$P(1)$**,$L_{i，j}$的取值为：<img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230919184226689.png" alt="image-20230919184226689" style="zoom:33%;" />

当输入0或1时，x的值的概率分布为：
$$
p(x\mid\omega_{1})={\frac{1}{\sqrt{2\pi}\,\sigma}}\,e^{-\frac{x^{2}}{2\sigma^{2}}}\qquad P(x\mid\omega_{2})={\frac{1}{\sqrt{2\pi}\,\sigma}}e^{-\frac{(x-1)^{2}}{2\sigma^{2}}}
$$
写出两类的平均风险，并比较：
$$
\frac{r_{1}(x)}{r_{2}(x)}=\frac{L_{11}p(x|\omega_{1})P(0)+L_{12}p(x|\omega_{2})P(1)}{L_{21}p(x|\omega_{1})P(0)+L_{22}p(x|\omega_{2})P(1)}
$$
如果$\frac{r_{1}(x)}{r_{2}(x)}<1$，意味着预测为$w_1$的风险更小，结果为$w_1$，输出$0$信号。反之则输出$1$信号。

#### 方法二（似然比与阈值）：

还是根据方法一的思路：

$r_1(x)=L_{11}p(x|\omega_{1})P(0)+L_{12}p(x|\omega_{2})P(1)$

$r_2(x)=L_{21}p(x|\omega_{1})P(0)+L_{22}p(x|\omega_{2})P(1)$

因此有：

$r_1(x)-r_2(x) = (L_{11}-L_{21})p(x|\omega_{1})P(0)+(L_{12}-L_{22})p(x|\omega_{2})P(1)$

可以得到：

<img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230919185908144.png" alt="image-20230919185908144" style="zoom:50%;" />

<img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230919185921027.png" alt="image-20230919185921027" style="zoom: 50%;" />

















# 第三章、判别函数

## 3.1 判别式分类器与生成式分类器

![img](https://cdn.nlark.com/yuque/0/2023/png/22608736/1695709751405-97d91630-58b7-41b5-89e4-fb31a7320b98.png)

判别式分类器：只关心情况的可能性，适合于绝大多数的情况。

生成式分类器：能够反映同类数据本身的相似度，但是不关心各分类的边界，相较于判别式分类器能够更快收敛。生成式模型能够应付存在隐变量的情况。

## 3.2 线性判别函数

略...

## 3.3 广义线性判别函数

线性判别函数实现简单但是无法处理复杂情况，而非线性判别函数能够处理复杂情况但是实现较困难。若能**将非线性判别函数转换为线性判别函数**则有利于模式分类的实现。

### 核心思想

训练集$\{x\}$ 在模型空间$x$中线性不可分，但是在更高维度模式空间$x*$中线性可分，x*的维数k高于x的维数n，择取：
$$
x*=(f_1(x),f_2(x),f_3(x),....,f_k(x)),k>n
$$
此时只要将模式x进行非线性变换，使之变换后得到维数更高的模式$x *$ ，就可以用线性判别函数来进行分类。

### 描述

![image-20230928104312886](https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230928104312886.png)

- 对于二次多项式情况：$d(x)=w_{11}x_{1}^{2}+w_{12}x_{1}x_{2}+w_{22}x_{2}^{2}+w_{1}x_{1}+w_{2}x_{2}+w_{3}$

​		   此时<img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230928104830062.png" alt="image-20230928104830062" style="zoom:67%;" />

- 对于n维的情况：$d(x)=\sum_{j=1}^{n}w_{j j}x_{j}^{2}+\sum_{j=1}^{n-1}\sum_{k=j+1}^{n}w_{j k}x_{j}x_{k}+\sum_{j=1}^{n}w_{j}x_{j}+w_{n+1}$

​			此时总项数维：$n+n(n-1)/2+n+1 = (n+1)(n+2)/2>n$

​			此时x*各分量一般化形式： $f_{i}(x)=x_{p_{1}}^{s}x_{p_{2}}^{t},p_{1},p_{2}=1,2,\cdots,n;\;s,t=0,1$

### 广义线性判别函数的意义

意义在于通过**非线性变换**将输入模式映射到一个高维特征空间中，使得在新的特征空间中模式更容易被线性判别函数分割。这种变换可以将原始的模式空间转换为一个更具可分性的特征空间，从而提高模式分类的准确性。

就如下面这个例子：

<img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230928105547246.png" alt="image-20230928105547246" style="zoom:80%;" />

原式：$d(x)=w^Tx$ 

现在令：$x*=(x^2,x,1)^T=(x_1,x_2,1)$ $w = (1,-(a+b),ab)$

<img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230928110008403.png" alt="image-20230928110008403" style="zoom:67%;" />

## 3.4 分段线性函数

### 出发点

线性判别函数简单有效但是无法适用于所有分类情况；广义线性判别函数能通过增大维度来得到线性判别，但是维度大量增加会导致计算复杂性增大。

因此引入分段线性判别函数的判别过程，**它比一般的线性判别函数的错误率小，但又比非线性判别函数简单**

例如下面这个例子：

![image-20230928110348254](https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230928110348254.png)

### 设计

设计分段线性判别函数的方式为：**最小距离分类法**

$u_1$和$u_2$为两类$w_1$和$w_2$的聚类中心（它代表着聚类中的一个代表性点或中心点，该点被认为是与同一聚类中的其他数据点最为接近的点），定义决策规则为：

<img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230928111339081.png" alt="image-20230928111339081" style="zoom:67%;" />

$||x-u_i||^2$  表示随机变量$x$到$u_i$的距离

![image-20230928111643057](https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230928111643057.png)

而对于各类交错分布的情况，可以可以运用聚类方法（例如k-means）将一些类分解成若干个子 类，再用最小距离分类，如下图所示：

<img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230928111734315.png" alt="image-20230928111734315" style="zoom:67%;" />

## 3.5 Fisher线性判别

### 原理概述

考虑把d维空间的样本投影到一条直线上，形成一维空间，即把维数压缩到一维，如何根据实际情况找到一条最好的、最易于分类的投影线，这就是Fisher判别方法所要解决的基本问题

为了得到这个最好的、最易于分类的投影线，需要：**使得同类样例的投影点尽可能接近、不同类样例的投影点尽可能远离**如下所示，给出了一个二维示意图：

<img src="https://pic3.zhimg.com/80/v2-e7e2482730034721a2b391b408613d6e_720w.webp" alt="img" style="zoom:67%;" />

上面二维示意图中的"+"、"-"分别代表正例和反例，椭圆表示数据簇的外轮廓，虚线表示投影，红色实心圆和红色实心三角形分别表示两类样本投影后的中心点。

> **那么如何理解$w^Tx$?**

答：w在这里可以理解为这条过原点直线的单位方向向量。而 $w^Tx$ ，可以直接利用向量的内积去几何形象地理解它，因此$w^Tx$当做$x$到$w$方向上投影点到原点的距离（原点的一侧为正，另一侧为负）。

![image-20231005170242594](https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20231005170242594.png)

![image-20230928121049007](https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230928121049007.png)

### Fisher准则函数的定义

把训练样本集划分为2个子集$X_0$和$X_1$,给出下面的参数：

- <font color='red'>各类样本均值向量</font> $:i=0,1\ ;\ \ u_{i}={\frac{1}{N_{i}}}\sum_{x\in X_{i}}x$

  各类样本均值向量代表某一类(i)中所有样本参数的均值，$w^Tu_i$就是某一类(i)所有样本到原点距离的平均值。

- <font color='red'>类内离散度矩阵</font> ：$S_{wi}=\sum_{x\in X_{i}}\left(x-u_{i}\right)\left(x-u_{i}\right)^{T}$

  $\sum_{x\in X_{i}}(w^{T}x-w^{T}u_{i})^{2}$  ，$w^Tx$ 是样本$x$到原点的距离、$w^Tu_i$是类 i 所有样本到原点的平均距离，因此该式是类 i 所有样本到原点距离的方差。现在我们将上面换个矩阵的写法：

  <img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230928123655370.png" alt="image-20230928123655370" style="zoom:67%;" />

  因此$w^TS_{bi}w$ 代表到原点距离的方差

- <font color='red'>总样本类内离散度矩阵</font>：$S_w=S_1+S_2$

- <font color='red'>样本类间散度矩阵</font> ：$S_b = (u_1-u_2)(u_1-u_2)^T$

之前提到，Fisher准则函数的目的是：**<font color='red'>在一维Y空间中各类样本尽可能分得开些，即希望两类均值之差越大越好，同时希望各类样本内部尽量密集，即希望类内离散度越小越好</font>**

- 首先是希望各类样本内部尽量密集：使得同类样例的投影点之间的距离尽可能接近，只需使得$J_1 = w^TS_ww$ 尽可能地小

- 其次是希望不同样本类样本投影点的距离尽可能地远：只有使得$J_2=w^TS_bw$ 尽可能地大

因此可以构造代价函数：
$$
J=\frac{J_{2}}{J_{1}}\,=\,\frac{w^{T}S_{b}w}{w^{T}S_{w}w}
$$
观察这个代价函数，其函数值只与直线的方向$w$有关而与在这个方向上距离是多少无关，因此只需考虑的是$\frac{J_2}{J_1}$ 的比例而无须得到他们的具体值，那么假定$J_1 = c \neq 0$  

此时根据拉格朗日常数法求代价函数的导数：

![image-20230928130149134](https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230928130149134.png)

![image-20230928130159708](https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230928130159708.png)

上述过程涉及到矩阵求导，最终得到结果：$w*=S_w^{-1}(u_1-u_2)$ ，其中$S_w^{-1}$是$S_w$矩阵的逆矩阵。

### 多类情况

前面是针对只有两个类的情况，假设类别变成多个，此时一维已经不能满足要求。假设此时有$C$个类别，需要$K$维（通常设K = C - 1）向量来做投影。

K 个投影向量$W= [w_1,w_2,...,w_k]$ ，投影后的结果为$y=[y_1,y_2,...,y_k]^T$
$$
y = W^Tx
$$
以下是Fisher线性判别多类情况的基本步骤：

1. 计算每个类别的<font color='red'>均值向量</font>：对于每个类别，计算其样本的平均值向量，表示为$μ_1、μ_2、...、μ_C$。

2. 计算<font color='red'>类内散布矩阵</font>（within-class scatter matrix）：类内散布矩阵用于度量类别内样本的散布程度。它可以通过计算每个类别内样本与其均值向量之间的差异来获得。对于第i个类别，其类内散布矩阵表示为$S{wi} = Σ(x_i - μ_i)(x_i - μ_i)^T$，其中xi是第i个类别的样本，μi是第i个类别的均值向量。

3. 计算<font color='red'>总样本散布矩阵</font>：$S_{w}=\sum_{j=1}^{C}S_{wj}$

4. 计算<font color='red'>类间散布矩阵</font>（between-class scatter matrix）：类间散布矩阵用于度量不同类别之间的差异性。它可以通过计算不同类别均值向量之间的差异来获得。类间散布矩阵表示为$S_b = Σ(Ni)(μ_i - μ)(μ_i - μ)T$，其中Ni是第i个类别的样本数量，μ是所有样本的平均值向量。

5. 计算投影方向：Fisher线性判别的目标是找到一个投影方向，使得投影后的样本在不同类别之间的散布最大化，同一类别内的散布最小化。通过求解广义特征值问题，可以得到w，使得w满足$S_w^{-1}S_b$的最大特征值对应的特征向量。

   ![image-20231001115204000](https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20231001115204000.png)

## 3.6感知器算法

### 感知器算法的由来

感知器算法（Perceptron Algorithm）是由Frank Rosenblatt于1957年提出的，它是一种基于神经元模型的机器学习算法。Rosenblatt受到生物神经元工作原理的启发，提出了感知器算法作为一种模拟神经元行为的方法。感知器算法在机器学习的发展历程中具有重要的地位，为后续的神经网络和支持向量机等算法奠定了基础。

### 感知器算法实现二分类

感知器算法的主要目标是实现二分类。其基本思想是通过一个**线性决策边界**将两个类别分开。算法通过迭代调整权重和阈值，使得对于每个输入样本，如果被正确分类，则权重保持不变，否则进行更新。具体实现步骤如下： 

1. 初始化权重向量和阈值。
2. 对于每个训练样本，计算输入向量与权重向量的内积，然后与阈值进行比较。
3. 如果内积大于阈值且样本标签为正类，则权重保持不变。
4. 如果内积小于等于阈值且样本标签为负类，则权重向量加上输入向量。
5. 如果内积小于等于阈值且样本标签为正类，则权重向量减去输入向量。
6. 重复上述步骤，直到所有样本都被正确分类或达到预定的迭代次数。

感知器算法使用阶跃函数或其他激活函数，将输入向量与权重向量的内积与阈值进行比较，得到最终的分类结果。下面通过一个简单的例子来说明感知器算法的二分类过程：

假设我们有一个二维的数据集，包含两个类别：红色圆圈和蓝色三角形。每个样本都有两个特征：x1和x2。

```plain
红色圆圈（正类）: (1, 2), (2, 3), (2, 1)
蓝色三角形（负类）: (-1, -1), (-2, -3), (-2, -1)
```

我们的目标是使用感知器算法找到一条直线作为决策边界，将红色圆圈和蓝色三角形分开。

首先，我们初始化权重向量w和阈值b。对于二维数据，权重向量w有两个分量，分别表示x1和x2的权重。我们可以将阈值b看作权重向量的第三个分量。

在开始迭代之前，我们可以随机初始化权重向量和阈值，例如：

```plain
权重向量 w = [0.5, -0.5]
阈值 b = 0
```

接下来，我们开始迭代处理样本。对于每个样本，我们计算输入向量x与权重向量w的内积，然后与阈值b进行比较。

以第一个样本 (1, 2) 为例：

```plain
内积 = (1 * 0.5) + (2 * -0.5) = 0
```

由于内积等于阈值，我们判断这个样本被正确分类。

接下来，我们处理第二个样本 (2, 3)：

```plain
内积 = (2 * 0.5) + (3 * -0.5) = 0.5 - 1.5 = -1
```

由于内积小于阈值，我们判断这个样本被错误分类。根据感知器算法的规则，我们需要更新权重向量和阈值。

更新规则如下：

```plain
权重向量 w = w + 学习率 * 输入向量
阈值 b = b + 学习率
```

假设学习率为1，我们可以进行更新：

```plain
权重向量 w = [0.5, -0.5] + [2, 3] = [2.5, 2.5]
阈值 b = 0 + 1 = 1
```

然后，我们继续处理下一个样本，重复上述步骤。

通过迭代处理所有样本，直到所有样本都被正确分类或达到预定的迭代次数。最终，我们会得到一个能够将红色圆圈和蓝色三角形分开的决策边界。

需要注意的是，感知器算法的收敛性要求数据集是线性可分的。如果数据集不是线性可分的，感知器算法可能无法收敛或产生较

### 感知器算法实现多分类

感知器算法最初是为二分类问题设计的，但可以通过一些策略将其扩展为多分类问题的解决方案。

####  一对多（One-vs-Rest）策略

在一对多策略中，对于有K个类别的多分类问题，我们训练K个感知器模型。每个模型将一个类别作为正类，而将其他K-1个类别作为负类。在预测阶段，对于一个新的输入样本，每个感知器模型都会产生一个分数，表示样本属于对应类别的概率。最后，选择具有最高概率的类别作为最终的分类结果。

####  一对一（One-vs-One）策略

在一对一策略中，对于有K个类别的多分类问题，我们训练K*(K-1)/2个感知器模型。每个模型只区分两个类别之间的差异，而忽略其他类别。在预测阶段，每个感知器模型都会对新的输入样本进行分类，最后通过投票或其他策略确定最终的分类结果。

这些方法可以用于将感知器算法扩展到多类分类问题。然而，对于大规模的多类分类问题，这些方法可能会导致模型数量庞大和计算复杂度增加的问题。在实际应用中，可以使用其他更高级的算法，如支持向量机（SVM）或深度学习模型（如神经网络）来处理多类分类问题。

###  感知器算法的局限性

##### 非线性可分问题

感知器算法只适用于线性可分的问题，这意味着存在一个超平面可以将不同类别的样本完全分开。对于非线性可分的问题，感知器算法无法收敛到满足要求的解。

为了解决非线性可分问题，可以使用一些技巧，如引入非线性的特征转换、使用核函数将数据映射到高维空间等。这些技术可以将非线性问题转化为线性可分问题，然后应用感知器算法进行分类。

------

##### 数据噪音

感知器算法对于包含噪音的数据可能无法收敛，或者产生较差的分类结果。当数据中存在噪音时，感知器算法可能会产生错误的更新，导致无法找到一个合适的决策边界。

为了应对数据噪音，可以采取一些方法来改进感知器算法的鲁棒性，如引入正则化项、增加迭代次数、使用更强大的分类器等。

------

##### 特征缩放和标准化

感知器算法对输入特征的缩放和标准化敏感。如果输入特征的尺度差异很大，可能会导致算法收敛缓慢或产生不稳定的结果。因此，在应用感知器算法之前，通常需要对输入数据进行预处理，以确保特征具有相似的尺度和分布。

常见的预处理方法包括特征缩放（例如将特征值归一化到0-1范围或使用标准化使其具有零均值和单位方差）和特征标准化（例如将离散特征编码为二进制指示变量）。

# 第四章、特征选择和提取

> 机器学习在本质上还是特征工程，数据和特征决定了机器学习的上线，而模型和算法只是逼近这个上限而已。																																															—— 吴恩达

特征选择和提取是构造模式识别系统时的一个关键问题，分类器设计时一直假定已给出了特征向量维数确定的样本集，其中**各样本的每一维都是该样本的一个特征**。特征的选择直接影响到分类器的设计及其性能

例如下面这个例子中：

![image-20231024135822708](https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20231024135822708.png)

如果不进行特征选择和提取，会导致数据空间的维度可能非常大， 随着空间维度的增长，数据点越来越分散，以至于距离和密度的概念变得越来越模糊，这会带来维**度灾难**，会导致：

1. **分类器性能下降**：当数据点在高维空间中变得分散时，不同类别之间的分界线变得模糊，导致分类器的性能下降。
2. **过拟合**：高维数据中存在的维度灾难可能导致过拟合问题。过拟合指的是模型在训练数据上表现良好，但在新数据上的泛化能力较差。当数据维度较高时，模型可能过于复杂地适应了训练数据中的噪声和随机变化，而无法捕捉到真正的数据模式和规律。
3. **计算复杂性增加**：高维数据的处理和计算复杂性也随之增加。在高维空间中进行特征选择、特征提取和模式识别的计算成本较高，算法的效率和速度可能受到影响。

![image-20231024140959005](https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20231024140959005.png)

为了设计出效果好的分类器，通常需要对原始的测量值集合进行分析，经过选择或变换处理，组成有效的识别特征；在保证一定分类精度的前提下，减少特征维数，即进行“降维”处理，使分类器实现快速、准确和高效的分类。

因此**特征选择和提取的关键在于**：<font color='red'>所提供的识别特征应具有很好的可分性，使分类器容易判别</font>。

- 因去掉模棱两可、不易判别的特征；
- 所提供的特征不要重复，即去掉那些相关性强且没有增加更多分类信息的特征。

实际上，特征选择和提取这一任务应在设计分类器之前进行：

![image-20231024141529135](https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20231024141529135.png)

<font color='red'>**特征选择**</font>：就是从$n$ 个度量值集合$\{x_1,x_2,...,x_n\}$ 中按照**某一准则**选取出供分类使用的子集，作为降维($m$ 维，$m < n$)的分类特征。

<font color='red'>**特征提取**</font>：就是使$\{x_1,x_2,...,x_n\}$ 通过某种变换，产生$m$个特征$\{y_1,y_2,..,y_m\}$ $(m<n)$，作为新的分类特征（或者称为二次特征）；

<font color='red'>**目的**</font>：为了在尽可能**保留识别信息的前提**下，**降低特征空间的维数**，已达到有效的分类。

以细胞自动识别为例：

- 通过图像输入得到一批包括正常细胞和异常细胞的图像，我们的任务是根据这些图像区分哪些细胞是正常的，哪些细胞是异常的；

- 找出一组能代表细胞性质的特征，为此可计算：

  细胞总面积、总光密度、胞核面积、核浆比、细胞形状、核内纹理 ...

这样产出的**原始特征**会有很多（几十甚至几百个），或者说**原始特征空间维数**很高，需要降低（或称压缩）维数以便分类：

- 一种方式是从原始特征中挑选出一些最有代表性的特征，称之为**特征选择**
- 另一种方式是用映射（或称变换）的方法把原始特征变换为较少的特征，称之为**特征提取** (如Fisher)。

## 特征选择

![image-20231024145454316](https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20231024145454316.png)

### 手工特征选择

这是一种简单但经验丰富的方法，通过领域知识和专业经验，人工选择那些认为对于分类任务最具有意义和区分性的特征。手工选择的特征依赖于领域专家的知识和直觉，可以快速实施，但可能无法充分利用数据中的信息。

手工选择会移除下列特征：

1. 冗余特征
2. 不相关特征
3. 质量差特征
4. 方差过小的特征

---

### 过滤式选择

这种方法通过对特征进行评估和排序，选择那些与目标变量相关性较高的特征。常用的过滤方式是设计一个<font color='red'>相关统计量</font>来度量特征的重要性，下面是一些常见的统计量：

![image-20231024145955908](https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20231024145955908.png)

过滤式特征选择根据单个特征与目标之间统计分数选择特征，速度快，但缺点是<font color='red'>没有考虑到特征之间的关联作用</font>

---

### 包裹式选择

这种方法将特征选择看作是一个搜索问题，通过尝**试不同的特征子集**，并使用分类器的性能作为评价指标，选择具有最佳性能的特征子集。常见的包装式选择方法包括递归特征消除（Recursive Feature Elimination，RFE）和遗传算法。包装式选择可以更准确地评估特征子集的性能，但计算复杂度较高



---

### 嵌入式选择  

这种方法将特征选择与分类器的训练过程融合在一起，通过在分类器训练过程中选择最佳的特征子集。常见的嵌入式选择方法包括L1正则化（如LASSO和岭回归）和决策树算法（如C4.5和Random Forest）。嵌入式选择可以考虑特征之间的相互影响，但计算复杂度较高。

## 离散K-L变换

K-L变换全称：Karhunen-Loeve变换（卡洛南-洛伊变换）

特征选择是在一定准则下，删掉某n-k个特征的做法并不十分理想，因为一般来说，原来的n个数据各自在不同程度上反映了识别对象的某些特征，简单地删去某些特征可能会丢失较多的有用信息。

如果将原来的特征做**正交变换**，获得的每个数据都是原来n个数据的线性组合，然后从新的数据中选出少数几个，使其尽可能多地反映各类模式之间的差异，而这些特征间又尽可能相互独立，则比单纯的选择方法更灵活、更有效

K-L变换就是一种适用于**任意概率密度函数**的正交变换。

# 第六章、有监督学习方法































