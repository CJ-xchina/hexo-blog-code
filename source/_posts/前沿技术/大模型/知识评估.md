---
title: 知识评估
date: 2023年10月16日15:09:04
    - AI
    - 课堂学习
    - 大模型
categories: 前沿技术
keywords:
description:
top_img:
comments:
cover:
toc:
toc_number:
toc_style_simple:
copyright:
copyright_author:
copyright_author_href:
copyright_url:
copyright_info:
mathjax:
katex:
aplayer:
highlight_shrink:
aside:
---

<meta name="referrer" content="no-referrer"/>



# Prompt-based Probing



## 提示词工程概述

### 什么是提示词？

简单来说**提示词(prompt)**是人与大模型交互的媒介。

就如同人与计算机之间的交互，从一开始的命令行，到后来的图像界面GUI，到现在的自然用户界面NUI，通过交互来让计算机明白用户的需求。同样如果将大模型视为一种强大的计算设备，那么提示词额就是一种新型的自然用户界面。

通过Prompt的输入，直接告诉模型我们要它干什么，模型便会干什么。

例如，我们可以问模型“用最多20个词总结下列文字”、“中国的首都在哪里？”、“判断下列文字的情感是正向还是负向”等等。我们输入的这些prompt，将会被模型识别、处理，最终输出为我们要的答案。



## 提示词设计与生成



### 具体方法

#### 手工提示 Handcraft Prompt



#### 优化离散提示 Optimized Discrete Prompt



#### 持续提示 Continual Prompt



#### 局限性 Limitations







# 拓展知识

## 输入嵌入（Input Embedding）

输入嵌入（Input Embedding）是指将离散的输入数据（例如单词、字符或其他符号）转换为连续的向量表示的过程。它是自然语言处理（NLP）和机器学习中常用的一种技术，用于将文本数据表示为计算机能够处理的数值形式。常见的输入嵌入方式有三种，分别是独热编码（One-Hot Encoding）、字符嵌入（Word Embedding）以及位置嵌入（Position Embedding）。

### One-Hot Encoding

假如我们要计算的文本中一共出现了4个词：猫、狗、牛、羊。向量里每一个位置都代表一个词。所以用 one-hot 来表示就是：

猫：［1，0，0，0］

狗：［0，1，0，0］

牛：［0，0，1，0］

羊：［0，0，0，1］

![img](https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/0*2gYJQEG0d_RhdNt8.png)

但是在实际情况中，文本中很可能出现成千上万个不同的词，这时候向量就会非常长。其中99%以上都是 0。

one-hot 的缺点如下：

1. 无法表达词语之间的关系
2. 这种过于稀疏的向量，导致计算和存储的效率都不高

### word embedding

word embedding 是文本表示的一类方法。跟 one-hot 编码和整数编码的目的一样，不过他有更多的优点。

词嵌入并不特指某个具体的算法，跟上面2种方式相比，这种方法有几个明显的优势：

1. 他可以将文本通过一个低维向量来表达，不像 one-hot 那么长。
2. 语意相似的词在向量空间上也会比较相近。
3. 通用性很强，可以用在不同的任务中。

再回顾上面的例子：

![img](https://miro.medium.com/v2/resize:fit:700/0*gnFApYnNhC6VNeIY.png)

两种主流的 word embedding 算法：

![img](https://miro.medium.com/v2/resize:fit:700/0*SbCj8AWFSRK4V09I.png)

> Word2vec是一种基于统计方法来获得词向量的方法，他是 2013 年由谷歌的 Mikolov 提出了一套新的词嵌入方法。
>
> 这种算法有2种训练模式：
>
> 1. 通过上下文来预测当前词
> 2. 通过当前词来预测上下文
>
> 想要详细了解 [Word2vec](https://easyai.tech/ai-definition/word2vec/)，可以看看这篇文章：《[一文看懂 Word2vec（基本概念+2种训练模型+5个优缺点）](https://easyai.tech/ai-definition/word2vec/)》

> GloVe 是对 Word2vec 方法的扩展，它将全局统计和 Word2vec 的基于上下文的学习结合了起来。

# 参考论文

## 老师论文

[The Life Cycle of Knowledge in Big Language Models: A Survey”](zotero://note/u/XRMZF39M/)

## 人工提示词

[Language Models as Knowledge Bases?](zotero://note/u/MVZWRJLV/)

## 优化离散提示

[(2020-11) AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts](zotero://note/u/JK2N73PR/)

[(12/2020) How Can We Know What Language Models Know?](zotero://note/u/EMH47RYN/)

## 连续提示

[(8/2023) GPT understands, too](zotero://note/u/FLTPP3NT/)

[(2021-06) Factual Probing Is [MASK\]: Learning vs. Learning to Recall](