---
title: 知识评估
date: 2023年10月16日15:09:04
    - AI
    - 课堂学习
    - 大模型
categories: 前沿技术
keywords:
description:
top_img:
comments:
cover:
toc:
toc_number:
toc_style_simple:
copyright:
copyright_author:
copyright_author_href:
copyright_url:
copyright_info:
mathjax:
katex:
aplayer:
highlight_shrink:
aside:
---

<meta name="referrer" content="no-referrer"/>



# 

# 拓展知识

## 输入嵌入（Input Embedding）

输入嵌入（Input Embedding）是指将离散的输入数据（例如单词、字符或其他符号）转换为连续的向量表示的过程。它是自然语言处理（NLP）和机器学习中常用的一种技术，用于将文本数据表示为计算机能够处理的数值形式。常见的输入嵌入方式有三种，分别是独热编码（One-Hot Encoding）、字符嵌入（Word Embedding）以及位置嵌入（Position Embedding）。

### One-Hot Encoding

假如我们要计算的文本中一共出现了4个词：猫、狗、牛、羊。向量里每一个位置都代表一个词。所以用 one-hot 来表示就是：

猫：［1，0，0，0］

狗：［0，1，0，0］

牛：［0，0，1，0］

羊：［0，0，0，1］

![img](https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/0*2gYJQEG0d_RhdNt8.png)

但是在实际情况中，文本中很可能出现成千上万个不同的词，这时候向量就会非常长。其中99%以上都是 0。

one-hot 的缺点如下：

1. 无法表达词语之间的关系
2. 这种过于稀疏的向量，导致计算和存储的效率都不高

### word embedding

word embedding 是文本表示的一类方法。跟 one-hot 编码和整数编码的目的一样，不过他有更多的优点。

词嵌入并不特指某个具体的算法，跟上面2种方式相比，这种方法有几个明显的优势：

1. 他可以将文本通过一个低维向量来表达，不像 one-hot 那么长。
2. 语意相似的词在向量空间上也会比较相近。
3. 通用性很强，可以用在不同的任务中。

再回顾上面的例子：

![img](https://miro.medium.com/v2/resize:fit:700/0*gnFApYnNhC6VNeIY.png)

两种主流的 word embedding 算法：

![img](https://miro.medium.com/v2/resize:fit:700/0*SbCj8AWFSRK4V09I.png)

> Word2vec是一种基于统计方法来获得词向量的方法，他是 2013 年由谷歌的 Mikolov 提出了一套新的词嵌入方法。
>
> 这种算法有2种训练模式：
>
> 1. 通过上下文来预测当前词
> 2. 通过当前词来预测上下文
>
> 想要详细了解 [Word2vec](https://easyai.tech/ai-definition/word2vec/)，可以看看这篇文章：《[一文看懂 Word2vec（基本概念+2种训练模型+5个优缺点）](https://easyai.tech/ai-definition/word2vec/)》

> GloVe 是对 Word2vec 方法的扩展，它将全局统计和 Word2vec 的基于上下文的学习结合了起来。