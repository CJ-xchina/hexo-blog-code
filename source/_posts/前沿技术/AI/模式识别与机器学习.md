---
title: 模式识别与机器学习
date: 2023年9月13日12:48:36
tags: 
    - AI
    - 课堂学习
categories: 前沿技术
keywords:
description: 
top_img: https://w.wallhaven.cc/full/yj/wallhaven-yjvxeg.jpg
comments:
cover: https://w.wallhaven.cc/full/yj/wallhaven-yjvxeg.jpg
toc: 
toc_number:
toc_style_simple:
copyright:
copyright_author:
copyright_author_href:
copyright_url:
copyright_info:
mathjax:
katex:
aplayer:
highlight_shrink:
aside:
---

<meta name="referrer" content="no-referrer"/>

# 概述

==模式==：在于时间和空间可观测的物体，可以区别其是否相同或相似。
==模式的直观特性==：

* 可观测性
* 可区分性
* 相似性

==模式识别与机器学习的目的==：利用计算机对对象进行分类，在错误概率最小的条件下，使识别的结果与客观物体相符合，即： Y = F(X) ，X为输入特征集，Y为机器识别类别的标号集，F是模式识别判定算法。
得到F的方法，就是通过大量训练数据学习，提升F预测结果。

==机器学习的概念==：研究如何构造理论、算法和计算机系统，让机器通过数据中学习后进行分类、识别事物、推理预测和预测未来等。

==模式识别系统目标==：在特征空间与解释空间之间找到一种映射关系，该关系成为假设。

* 特征空间：从模式得到的对分类有用的度量、属性或基元构成的空间。
* 解释空间：所属类别的集合。

获得假说的两种方式：

1. 监督学习、概念驱动或归纳假说
2. 非监督学习、数据驱动或演绎假说

# 二、生成式分类器

## 2.1 贝叶斯判别准则

在给定观测值$x$的情况下，判断其属于$w_1$类还是$w_2$，作出某次判断时的错误率是：
$$
P(error|x)=\begin{cases}P(\omega_{1}|x),x\in\omega_{2}\\P(\omega_{2}|x),x\in\omega_{1}\end{cases}
$$
在贝叶斯判别准则下，若$P( \omega _ {1}  |x)>P(  \omega _ {2}  |x)$ 则判定类别$x\in w_1$，否则判别$x\in w_2$ ，该判别准则也就是要确定$x$是属于哪一类，这要取决于$x$来自哪一类的概率更大，即：

<img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230919173027970.png" alt="image-20230919173027970" style="zoom: 67%;" />

回顾一下**全概率公式**，设样本空间有随机事件$w_1，w_2，w_3，....w_n$，这$n$个随机事件两两不相容，即$w_1w_2=\phi$，且满足$\bigcup_{i=1}^{n}w_i=\Omega$，设$x$为$\Omega$中的一个事件，则全概率公式为：
$$
P(x) = \sum_{i=1}^{n}P(x|w_i)P(w_i)
$$
回顾一下贝叶斯公式，样本空间有随机事件$w_1，w_2，w_3，....w_n$，这$n$个随机事件两两不相容，即$w_1w_2=\phi$，且满足$\bigcup_{i=1}^{n}w_i=\Omega$，设$x$为$\Omega$中的一个事件，贝叶斯公式为：
$$
P(w_i|x)=\frac{P(x|w_i)P(w_i)}{\sum_{j=1}^{n}P(x|w_j)P(w_j)}=\frac{P(x|w_i)P(w_i)}{P(x)}
$$
根据贝叶斯公式：$P(  \omega _ {1}  |x) = \frac{P(x|w_1)P(w_1)}{P(x)}$  $P(  \omega _ {2}  |x) = \frac{P(x|w_2)P(w_2)}{P(x)}$，而两项中$P(x)$均为全概率公式相等，因此比贝叶斯判别式为：

<img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230919180132020.png" alt="image-20230919180132020" style="zoom: 50%;" />

化为：

<img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230919180553440.png" alt="image-20230919180553440" style="zoom:67%;" />

其中$P(x|w_i)$为**似然函数** ，表示特征向量x来自类别$ω_i$的概率，图中还有似然比与判决阈值的定义。

## 2.2 贝叶斯最小风险判别

### 代价矩阵

如果分类器判别$x$是属于$w_j$类，但它实际上来自$w_i$类，也就是说分类器判断失败，错误的分类会产生'损失'或'代价'，将代价存储在一个矩阵$L_{i，j}$中，$L_{i，j}$代表分类器判定为$w_j$类其实其属于$w_i$类这种情况所产生的代价，更具体来说:

- $L_{i，j}$是个矩阵，其中$i$表示样本的真实类，$j$表示分类器预测的类。
- 当$i=j$时，分类正确，$L_{i，j}$取 0 或负值，表示没有损失。
- 当$i≠j$时，分类错误$L_{i，j}$取正值，其数值表示错误分类的程度。

例如:

- 对数字识别问题来说，将5错误分类为3的损失可能比将5错误分类为0的损失小。
- 对病人诊断问题来说，将癌症患者错误判断为普通感冒的损失会比相反情况要大。

所以$L_{i，j}$可以对应不同类型的错误设置不同的“代价值”，从而形成一个“损失矩阵”。

通过最小平均条件风险进行分类，实际上是在考虑各种错误的$L_{i，j}$值，选择分类误差最小的方案。这样可以有效减轻高风险错误的影响。

所以简单来说，$L_{i，j}$反映了分类器在不同类型分类错误下的“损失级别”，是贝叶斯最小风险分类的一个重要参数。

---

### 平均条件风险

**平均条件风险(average conditional risk)**：样本$x$共有$M$种可供选择的类别，其所属真实类为$w_j$，将其分类到类$w_i$时的平均风险，也即：
$$
r_j=  \sum _ {i=1}^ {M} L_ {ij}  P(  w_{i}|x) = \frac{\sum^{M}_{i=1}L_{ij}P(x|w_i)P(w_i)}{P(x)}
$$
换句话说，平均条件风险考虑了:

1. 各种错误分类的损失程度($L_{ij}$表示)
2. 不同错误发生的概率($P(w_i|x)$表示错误分类到$w_i$的概率)
3. 对所有可能错误进行加权求和

通常来说，最小平均条件风险分类器就是：对每个样本$x$计算所有类的平均条件风险$r_j，$将样本$x$指定为平均条件风险最小的那一个类。

### 例题解析

题目：

![image-20230919183559815](https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230919183559815.png)

- 信道输入信号只可能为0或1。
- 信道传输过程中会加入高斯噪声。噪声的均值是0,方差是$σ^2$。
- 信道输出为x。
- **高斯分布(Gaussian Distribution)**也称正态分布

解析：

#### **方法一（平均条件风险）：**

噪声是服从$N(0, σ^2)$高斯分布，输入为0时,输出x ~ $N(0, σ^2)$;输入为1时,输出x ~$ N(1, σ^2)$

定义：**$w_1$：输出为0**；**$w_2$：输出为1**。用贝叶斯判别条件分析：**设信号送$0$的先验概率为**
**$P(0)$,送$1$的先验概率为$P(1)$**,$L_{i，j}$的取值为：<img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230919184226689.png" alt="image-20230919184226689" style="zoom:33%;" />

当输入0或1时，x的值的概率分布为：
$$
p(x\mid\omega_{1})={\frac{1}{\sqrt{2\pi}\,\sigma}}\,e^{-\frac{x^{2}}{2\sigma^{2}}}\qquad P(x\mid\omega_{2})={\frac{1}{\sqrt{2\pi}\,\sigma}}e^{-\frac{(x-1)^{2}}{2\sigma^{2}}}
$$
写出两类的平均风险，并比较：
$$
\frac{r_{1}(x)}{r_{2}(x)}=\frac{L_{11}p(x|\omega_{1})P(0)+L_{12}p(x|\omega_{2})P(1)}{L_{21}p(x|\omega_{1})P(0)+L_{22}p(x|\omega_{2})P(1)}
$$
如果$\frac{r_{1}(x)}{r_{2}(x)}<1$，意味着预测为$w_1$的风险更小，结果为$w_1$，输出$0$信号。反之则输出$1$信号。

#### 方法二（似然比与阈值）：

还是根据方法一的思路：

$r_1(x)=L_{11}p(x|\omega_{1})P(0)+L_{12}p(x|\omega_{2})P(1)$

$r_2(x)=L_{21}p(x|\omega_{1})P(0)+L_{22}p(x|\omega_{2})P(1)$

因此有：

$r_1(x)-r_2(x) = (L_{11}-L_{21})p(x|\omega_{1})P(0)+(L_{12}-L_{22})p(x|\omega_{2})P(1)$

可以得到：

<img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230919185908144.png" alt="image-20230919185908144" style="zoom:50%;" />

<img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230919185921027.png" alt="image-20230919185921027" style="zoom: 50%;" />

















# 第三章 判别函数

## 3.1 判别式分类器与生成式分类器

![img](https://cdn.nlark.com/yuque/0/2023/png/22608736/1695709751405-97d91630-58b7-41b5-89e4-fb31a7320b98.png)

判别式分类器：只关心情况的可能性，适合于绝大多数的情况。

生成式分类器：能够反映同类数据本身的相似度，但是不关心各分类的边界，相较于判别式分类器能够更快收敛。生成式模型能够应付存在隐变量的情况。

## 3.2 线性判别函数

略...

## 3.3 广义线性判别函数

线性判别函数实现简单但是无法处理复杂情况，而非线性判别函数能够处理复杂情况但是实现较困难。若能**将非线性判别函数转换为线性判别函数**则有利于模式分类的实现。

### 核心思想

训练集$\{x\}$ 在模型空间$x$中线性不可分，但是在更高维度模式空间$x*$中线性可分，x*的维数k高于x的维数n，择取：
$$
x*=(f_1(x),f_2(x),f_3(x),....,f_k(x)),k>n
$$
此时只要将模式x进行非线性变换，使之变换后得到维数更高的模式$x *$ ，就可以用线性判别函数来进行分类。

### 描述

![image-20230928104312886](https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230928104312886.png)

- 对于二次多项式情况：$d(x)=w_{11}x_{1}^{2}+w_{12}x_{1}x_{2}+w_{22}x_{2}^{2}+w_{1}x_{1}+w_{2}x_{2}+w_{3}$

​		   此时<img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230928104830062.png" alt="image-20230928104830062" style="zoom:67%;" />

- 对于n维的情况：$d(x)=\sum_{j=1}^{n}w_{j j}x_{j}^{2}+\sum_{j=1}^{n-1}\sum_{k=j+1}^{n}w_{j k}x_{j}x_{k}+\sum_{j=1}^{n}w_{j}x_{j}+w_{n+1}$

​			此时总项数维：$n+n(n-1)/2+n+1 = (n+1)(n+2)/2>n$

​			此时x*各分量一般化形式： $f_{i}(x)=x_{p_{1}}^{s}x_{p_{2}}^{t},p_{1},p_{2}=1,2,\cdots,n;\;s,t=0,1$

### 广义线性判别函数的意义

意义在于通过**非线性变换**将输入模式映射到一个高维特征空间中，使得在新的特征空间中模式更容易被线性判别函数分割。这种变换可以将原始的模式空间转换为一个更具可分性的特征空间，从而提高模式分类的准确性。

就如下面这个例子：

<img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230928105547246.png" alt="image-20230928105547246" style="zoom:80%;" />

原式：$d(x)=w^Tx$ 

现在令：$x*=(x^2,x,1)^T=(x_1,x_2,1)$ $w = (1,-(a+b),ab)$

<img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230928110008403.png" alt="image-20230928110008403" style="zoom:67%;" />

## 3.4 分段线性函数

### 出发点

线性判别函数简单有效但是无法适用于所有分类情况；广义线性判别函数能通过增大维度来得到线性判别，但是维度大量增加会导致计算复杂性增大。

因此引入分段线性判别函数的判别过程，**它比一般的线性判别函数的错误率小，但又比非线性判别函数简单**

例如下面这个例子：

![image-20230928110348254](https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230928110348254.png)

### 设计

设计分段线性判别函数的方式为：**最小距离分类法**

$u_1$和$u_2$为两类$w_1$和$w_2$的聚类中心（它代表着聚类中的一个代表性点或中心点，该点被认为是与同一聚类中的其他数据点最为接近的点），定义决策规则为：

<img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230928111339081.png" alt="image-20230928111339081" style="zoom:67%;" />

$||x-u_i||^2$  表示随机变量$x$到$u_i$的距离

![image-20230928111643057](https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230928111643057.png)

而对于各类交错分布的情况，可以可以运用聚类方法（例如k-means）将一些类分解成若干个子 类，再用最小距离分类，如下图所示：

<img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230928111734315.png" alt="image-20230928111734315" style="zoom:67%;" />

## 3.5 Fisher线性判别

### 原理概述

考虑把d维空间的样本投影到一条直线上，形成一维空间，即把维数压缩到一维，如何根据实际情况找到一条最好的、最易于分类的投影线，这就是Fisher判别方法所要解决的基本问题

为了得到这个最好的、最易于分类的投影线，需要：**使得同类样例的投影点尽可能接近、不同类样例的投影点尽可能远离**如下所示，给出了一个二维示意图：

<img src="https://pic3.zhimg.com/80/v2-e7e2482730034721a2b391b408613d6e_720w.webp" alt="img" style="zoom:67%;" />

上面二维示意图中的"+"、"-"分别代表正例和反例，椭圆表示数据簇的外轮廓，虚线表示投影，红色实心圆和红色实心三角形分别表示两类样本投影后的中心点。

**那么如何理解$w^Tx$?**

答：w在这里可以理解为这条过原点直线的单位方向向量。而 $w^Tx$ ，可以直接利用向量的内积去几何形象地理解它，因此$w^Tx$当做$x$到$w$方向上投影点到原点的距离（原点的一侧为正，另一侧为负）。
$$
\begin{array}{l}{{w^{T}x=<w,x>}}\\ {{=\,|w|\ast|x|\ast c o s(\theta)}}\\ {{=\,|x|\ast c o s(\theta)}}\end{array}
$$
![image-20230928121049007](https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230928121049007.png)

### Fisher准则函数的定义

把训练样本集划分为2个子集$X_0$和$X_1$,给出下面的参数：

- <font color='red'>各类样本均值向量</font> $:i=0,1\ ;\ \ u_{i}={\frac{1}{N_{i}}}\sum_{x\in X_{i}}x$

  各类样本均值向量代表某一类(i)中所有样本参数的均值，$w^Tu_i$就是某一类(i)所有样本到原点距离的平均值。

- <font color='red'>样本类内偏差</font> ：$S_{wi}=\sum_{x\in X_{i}}\left(x-u_{i}\right)\left(x-u_{i}\right)^{T}$

  $\sum_{x\in X_{i}}(w^{T}x-w^{T}u_{i})^{2}$  ，$w^Tx$ 是样本$x$到原点的距离、$w^Tu_i$是类 i 所有样本到原点的平均距离，因此该式是类 i 所有样本到原点距离的方差。现在我们将上面换个矩阵的写法：

  <img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230928123655370.png" alt="image-20230928123655370" style="zoom:67%;" />

  因此$w^TS_{bi}$

- <font color='red'>总样本类内离散度矩阵</font>：$S_w=S_1+S_2$

- <font color='red'>样本类内散度矩阵</font> ：$S_b = (u_1-u_2)(u_1-u_2)^T$

之前提到，Fisher准则函数的目的是：**在一维Y空间中各类样本尽可能分得开些，即希望两类均值之差越大越好，同时希望各类样本内部尽量密集，即希望类内离散度越小越好**

- 首先是希望各类样本内部尽量密集：使得同类样例的投影点之间的距离尽可能接近，只需使得$J_1 = w^TS_ww$ 尽可能地小

- 其次是希望不同样本类样本投影点的距离尽可能地远：只有使得$J_2=w^TS_bw$ 尽可能地大

因此可以构造代价函数：
$$
J=\frac{J_{2}}{J_{1}}\,=\,\frac{w^{T}S_{b}w}{w^{T}S_{w}w}
$$
观察这个代价函数，其函数值只与直线的方向$w$有关而与







