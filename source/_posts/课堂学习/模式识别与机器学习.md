---
title: 模式识别与机器学习
date: 2023年9月13日12:48:36
tags: 
    - AI
categories: 课堂学习
keywords:
description: 
top_img: https://w.wallhaven.cc/full/yj/wallhaven-yjvxeg.jpg
comments:
cover: https://w.wallhaven.cc/full/yj/wallhaven-yjvxeg.jpg
toc: 
toc_number:
toc_style_simple:
copyright:
copyright_author:
copyright_author_href:
copyright_url:
copyright_info:
mathjax:
katex:
aplayer:
highlight_shrink:
aside:
---

<meta name="referrer" content="no-referrer"/>

# 概述

==模式==：在于时间和空间可观测的物体，可以区别其是否相同或相似。
==模式的直观特性==：

* 可观测性
* 可区分性
* 相似性

==模式识别与机器学习的目的==：利用计算机对对象进行分类，在错误概率最小的条件下，使识别的结果与客观物体相符合，即： Y = F(X) ，X为输入特征集，Y为机器识别类别的标号集，F是模式识别判定算法。
得到F的方法，就是通过大量训练数据学习，提升F预测结果。

==机器学习的概念==：研究如何构造理论、算法和计算机系统，让机器通过数据中学习后进行分类、识别事物、推理预测和预测未来等。

==模式识别系统目标==：在特征空间与解释空间之间找到一种映射关系，该关系成为假设。

* 特征空间：从模式得到的对分类有用的度量、属性或基元构成的空间。
* 解释空间：所属类别的集合。

获得假说的两种方式：

1. 监督学习、概念驱动或归纳假说
2. 非监督学习、数据驱动或演绎假说

# 二、生成式分类器

## 2.1 贝叶斯判别准则

在给定观测值$x$的情况下，判断其属于$w_1$类还是$w_2$，作出某次判断时的错误率是：
$$
P(error|x)=\begin{cases}P(\omega_{1}|x),x\in\omega_{2}\\P(\omega_{2}|x),x\in\omega_{1}\end{cases}
$$
在贝叶斯判别准则下，若$P( \omega _ {1}  |x)>P(  \omega _ {2}  |x)$ 则判定类别$x\in w_1$，否则判别$x\in w_2$ ，该判别准则也就是要确定$x$是属于哪一类，这要取决于$x$来自哪一类的概率更大，即：

<img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230919173027970.png" alt="image-20230919173027970" style="zoom: 67%;" />

回顾一下**全概率公式**，设样本空间有随机事件$w_1，w_2，w_3，....w_n$，这$n$个随机事件两两不相容，即$w_1w_2=\phi$，且满足$\bigcup_{i=1}^{n}w_i=\Omega$，设$x$为$\Omega$中的一个事件，则全概率公式为：
$$
P(x) = \sum_{i=1}^{n}P(x|w_i)P(w_i)
$$
回顾一下贝叶斯公式，样本空间有随机事件$w_1，w_2，w_3，....w_n$，这$n$个随机事件两两不相容，即$w_1w_2=\phi$，且满足$\bigcup_{i=1}^{n}w_i=\Omega$，设$x$为$\Omega$中的一个事件，贝叶斯公式为：
$$
P(w_i|x)=\frac{P(x|w_i)P(w_i)}{\sum_{j=1}^{n}P(x|w_j)P(w_j)}=\frac{P(x|w_i)P(w_i)}{P(x)}
$$
根据贝叶斯公式：$P(  \omega _ {1}  |x) = \frac{P(x|w_1)P(w_1)}{P(x)}$  $P(  \omega _ {2}  |x) = \frac{P(x|w_2)P(w_2)}{P(x)}$，而两项中$P(x)$均为全概率公式相等，因此比贝叶斯判别式为：

<img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230919180132020.png" alt="image-20230919180132020" style="zoom: 50%;" />

化为：

<img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230919180553440.png" alt="image-20230919180553440" style="zoom:67%;" />

其中$P(x|w_i)$为**似然函数** ，表示特征向量x来自类别$ω_i$的概率，图中还有似然比与判决阈值的定义。

## 2.2 贝叶斯最小风险判别

### 代价矩阵

如果分类器判别$x$是属于$w_j$类，但它实际上来自$w_i$类，也就是说分类器判断失败，错误的分类会产生'损失'或'代价'，将代价存储在一个矩阵$L_{i，j}$中，$L_{i，j}$代表分类器判定为$w_j$类其实其属于$w_i$类这种情况所产生的代价，更具体来说:

- $L_{i，j}$是个矩阵，其中$i$表示样本的真实类，$j$表示分类器预测的类。
- 当$i=j$时，分类正确，$L_{i，j}$取 0 或负值，表示没有损失。
- 当$i≠j$时，分类错误$L_{i，j}$取正值，其数值表示错误分类的程度。

例如:

- 对数字识别问题来说，将5错误分类为3的损失可能比将5错误分类为0的损失小。
- 对病人诊断问题来说，将癌症患者错误判断为普通感冒的损失会比相反情况要大。

所以$L_{i，j}$可以对应不同类型的错误设置不同的“代价值”，从而形成一个“损失矩阵”。

通过最小平均条件风险进行分类，实际上是在考虑各种错误的$L_{i，j}$值，选择分类误差最小的方案。这样可以有效减轻高风险错误的影响。

所以简单来说，$L_{i，j}$反映了分类器在不同类型分类错误下的“损失级别”，是贝叶斯最小风险分类的一个重要参数。

---

### 平均条件风险

**平均条件风险(average conditional risk)**：样本$x$共有$M$种可供选择的类别，其所属真实类为$w_j$，将其分类到类$w_i$时的平均风险，也即：
$$
r_j=  \sum _ {i=1}^ {M} L_ {ij}  P(  w_{i}|x) = \frac{\sum^{M}_{i=1}L_{ij}P(x|w_i)P(w_i)}{P(x)}
$$
换句话说，平均条件风险考虑了:

1. 各种错误分类的损失程度($L_{ij}$表示)
2. 不同错误发生的概率($P(w_i|x)$表示错误分类到$w_i$的概率)
3. 对所有可能错误进行加权求和

通常来说，最小平均条件风险分类器就是：对每个样本$x$计算所有类的平均条件风险$r_j，$将样本$x$指定为平均条件风险最小的那一个类。

### 例题解析

题目：

![image-20230919183559815](https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230919183559815.png)

- 信道输入信号只可能为0或1。
- 信道传输过程中会加入高斯噪声。噪声的均值是0,方差是$σ^2$。
- 信道输出为x。
- **高斯分布(Gaussian Distribution)**也称正态分布

解析：

#### **方法一（平均条件风险）：**

噪声是服从$N(0, σ^2)$高斯分布，输入为0时,输出x ~ $N(0, σ^2)$;输入为1时,输出x ~$ N(1, σ^2)$

定义：**$w_1$：输出为0**；**$w_2$：输出为1**。用贝叶斯判别条件分析：**设信号送$0$的先验概率为**
**$P(0)$,送$1$的先验概率为$P(1)$**,$L_{i，j}$的取值为：<img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230919184226689.png" alt="image-20230919184226689" style="zoom:33%;" />

当输入0或1时，x的值的概率分布为：
$$
p(x\mid\omega_{1})={\frac{1}{\sqrt{2\pi}\,\sigma}}\,e^{-\frac{x^{2}}{2\sigma^{2}}}\qquad P(x\mid\omega_{2})={\frac{1}{\sqrt{2\pi}\,\sigma}}e^{-\frac{(x-1)^{2}}{2\sigma^{2}}}
$$
写出两类的平均风险，并比较：
$$
\frac{r_{1}(x)}{r_{2}(x)}=\frac{L_{11}p(x|\omega_{1})P(0)+L_{12}p(x|\omega_{2})P(1)}{L_{21}p(x|\omega_{1})P(0)+L_{22}p(x|\omega_{2})P(1)}
$$
如果$\frac{r_{1}(x)}{r_{2}(x)}<1$，意味着预测为$w_1$的风险更小，结果为$w_1$，输出$0$信号。反之则输出$1$信号。

#### 方法二（似然比与阈值）：

还是根据方法一的思路：

$r_1(x)=L_{11}p(x|\omega_{1})P(0)+L_{12}p(x|\omega_{2})P(1)$

$r_2(x)=L_{21}p(x|\omega_{1})P(0)+L_{22}p(x|\omega_{2})P(1)$

因此有：

$r_1(x)-r_2(x) = (L_{11}-L_{21})p(x|\omega_{1})P(0)+(L_{12}-L_{22})p(x|\omega_{2})P(1)$

可以得到：

<img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230919185908144.png" alt="image-20230919185908144" style="zoom:50%;" />

<img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230919185921027.png" alt="image-20230919185921027" style="zoom: 50%;" />



























